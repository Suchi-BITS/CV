{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCavg-cjnNZd"
      },
      "source": [
        "# üå† Improving RAG by Optimizing Retrieval and Reranking Models\n",
        "\n",
        "In this tutorial, we will show how to improve a RAG (Retrieval Augmented Generation) model by optimizing the retrieval and reranking models. For this purpose, we will use the `ArgillaTrainer` to fine-tune a `bi-encoder` and `cross-encoder` on two datasets with our own data.\n",
        "\n",
        "The steps are as follows:\n",
        "\n",
        "* üìù Set up a QA pipeline with RAG using [Haystack](https://haystack.deepset.ai/overview/intro)\n",
        "* üóÉÔ∏è Get the answers and contexts to create our own datasets\n",
        "* üì© Create the Argilla datasets and push them to the `Argilla UI`\n",
        "* üí´ Fine-tune the `bi-encoder` and `cross-encoder`\n",
        "* üåå Use our fine-tuned models to improve the original RAG model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc4Rwr6CnNZf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "**LLMs** are a reality in our day-to-day lives. They are used in search engines, chatbots, and question-answering systems. However, they are not perfect. They often produce responses that are not relevant, accurate, or verifiable. To solve this problem, RAG (Retrieval-Agumented Generation) was introduced.\n",
        "\n",
        "**RAG** is a framework that improves the quality of the responses using a pre-trained LLM and a retrieval model. This one is used to retrieve relevant information from a knowledge base (the web or your documents) which makes it more trustworthy for the user. In addition, RAG solves the common LLMs drawbacks as it can provide up-to-date and domain-specific data (even citing its sources) and it is more efficient and affordable (no need to retrain models from scratch).\n",
        "\n",
        "In order to optimize the retrieval model, a **sentence similarity model** can be used. Why? To improve the accuracy and relevance of the retrieved information by finding the user's intent. This is done by transforming the text into embeddings (vectors representing the semantic information) and computing the similarity between those so that the meaning of the input text can be 'understood'.\n",
        "\n",
        "In this tutorial, we will fine-tune a sentence similarity model with a bi-encoder (faster but less accurate) and a cross-encoder (slower but more accurate). The **bi-encoder** creates sentence embeddings for the data and the query, and then compares them by computing the similarity between vectors. The **cross-encoder** does not use sentence embeddings but classifies the data pairs and outputs a value indicating the similarity between them. They can be used independently in a retriever or together as shown in the image below where the retrieval is the the initial step and involves searching through a vast dataset or collection to identify a subset of candidate documents, passages, or sentences that are potentially relevant to a given query or information requirement. Following this, the reranking phase takes place, where the candidates initially retrieved undergo reassessment and are reorganized based on their actual relevance to the query.\n",
        "\n",
        "![rag-diagram.jpg](attachment:rag-diagram.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GPNbw-RnNZg"
      },
      "source": [
        "## Running Argilla\n",
        "\n",
        "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
        "\n",
        "\n",
        "**Deploy Argilla on Hugging Face Spaces**: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
        "\n",
        "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/login?next=%2Fnew-space%3Ftemplate%3Dargilla%2Fargilla-template-space)\n",
        "\n",
        "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
        "\n",
        "\n",
        "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.html). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
        "\n",
        "For more information on deployment options, please check the Deployment section of the documentation.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "\n",
        "Tip\n",
        "    \n",
        "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
        "\n",
        "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
        "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter Notebook tool of your choice.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyNNZ_dKnNZh"
      },
      "source": [
        "## Set up the Environment\n",
        "\n",
        "To complete this tutorial, you will need to install the Argilla client and a few third-party libraries using `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRgT4xxonNZi"
      },
      "outputs": [],
      "source": [
        "# %pip install --upgrade pip\n",
        "%pip install argilla -qqq\n",
        "%pip install datasets\n",
        "%pip install sentence-transformers\n",
        "%pip install farm-haystack[colab,faiss,inference]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rcVmNkqnNZj"
      },
      "source": [
        "Let's make the needed imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJYZMTLOnNZj"
      },
      "outputs": [],
      "source": [
        "import argilla as rg\n",
        "from argilla.feedback import TrainingTask\n",
        "from argilla.feedback import ArgillaTrainer\n",
        "\n",
        "import random\n",
        "import locale\n",
        "import re\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from haystack.document_stores import FAISSDocumentStore\n",
        "from haystack.nodes import PreProcessor, TextConverter, EmbeddingRetriever, PromptNode, PromptTemplate, AnswerParser, SentenceTransformersRanker\n",
        "from haystack.pipelines import Pipeline\n",
        "from haystack.pipelines.standard_pipelines import TextIndexingPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvTcsmZ3nNZj"
      },
      "source": [
        "If you are running Argilla using the Docker quickstart image or a public Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7UEpz1tnNZk"
      },
      "outputs": [],
      "source": [
        "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
        "# Replace api_key if you configured a custom API key\n",
        "# Replace workspace with the name of your workspace\n",
        "rg.init(\n",
        "    api_url=\"http://localhost:6900\",\n",
        "    api_key=\"owner.apikey\",\n",
        "    workspace=\"admin\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QU8YqPDnNZk"
      },
      "source": [
        "If you're running a private Hugging Face Space, you will also need to set the [HF_TOKEN](https://huggingface.co/settings/tokens) as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfAHVGt-nNZk"
      },
      "outputs": [],
      "source": [
        "# # Set the HF_TOKEN environment variable\n",
        "# import os\n",
        "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
        "\n",
        "# # Replace api_url with the url to your HF Spaces URL\n",
        "# # Replace api_key if you configured a custom API key\n",
        "# rg.init(\n",
        "#     api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
        "#     api_key=\"admin.apikey\",\n",
        "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMXwL4TKnNZk"
      },
      "source": [
        "### Enable Telemetry\n",
        "\n",
        "We gain valuable insights from how you interact with our tutorials. To improve ourselves in offering you the most suitable content, using the following lines of code will help us understand that this tutorial is serving you effectively. Though this is entirely anonymous, you can choose to skip this step if you prefer. For more info, please check out the [Telemetry](../../reference/telemetry.md) page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_d8gVRSnNZl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from argilla.utils.telemetry import tutorial_running\n",
        "    tutorial_running()\n",
        "except ImportError:\n",
        "    print(\"Telemetry is introduced in Argilla 1.20.0 and not found in the current installation. Skipping telemetry.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPTdNaY6nNZl"
      },
      "source": [
        "## The Datasets\n",
        "\n",
        "The available datasets for sentence similarity can be of various types: pairs of positive similar sentences ([Sentence Compression](https://huggingface.co/datasets/embedding-data/sentence-compression)), for Natural Language Inference ([snli](https://huggingface.co/datasets/snli)), with a label for the sentence ([QQP_triplets](https://huggingface.co/datasets/embedding-data/QQP_triplets)), etc. You can find more information about the different types of datasets [here](https://huggingface.co/blog/how-to-train-sentence-transformers).\n",
        "\n",
        "In this example, we want to create two with our own data with pairs of sentences and a label after the annotation: one to optimize the retrieval model comparing the query and the context and the other to optimize the reranking model comparing the contexts. For that, we will use the generative QA pipeline with RAG of Haystack to get the context and answers from a knowledge base. Then, we will upload them to Argilla using the corresponding `FeedbackDatasetTemplates` to work in the Argilla UI.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X0hzZ-WnNZl"
      },
      "source": [
        "### Generating Responses\n",
        "\n",
        "[Haystack](https://haystack.deepset.ai/) stands as an open-source framework, offering the means to construct end-to-end pipelines for various NLP tasks. It is model-agnostic and can be used for tasks such as question answering, document search, and summarization. In this tutorial, we will use Haystack to generate responses for our datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGALuW8KnNZl"
      },
      "source": [
        "First, we download from HuggingFace the following [dataset](https://huggingface.co/datasets/argilla/cloud_assistant_questions) with questions about the Argilla Cloud which was created in this [tutorial](../../tutorials/feedback/fine-tuning-openai-rag-feedback.html). Then, we download the text file that will be used as the input data for the RAG model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PGgE3B0nNZl"
      },
      "outputs": [],
      "source": [
        "# Load the questions\n",
        "dataset = load_dataset(\"argilla/cloud_assistant_questions\")\n",
        "\n",
        "# Download the document for RAG\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\" # Run it if UTF-8 encoding error\n",
        "!curl https://huggingface.co/datasets/argilla/cloud_assistant_questions/raw/main/argilla_cloud.txt > argilla_cloud.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7uTWZLZnNZl"
      },
      "source": [
        "Now, let's preprocess our document using the predefined [TextIndexingPipeline](https://docs.haystack.deepset.ai/docs/ready_made_pipelines#textindexingpipeline). This pipeline allow us to initialize the `DocumentStore` (the database for the retriever), the `PreProcessor` (to clean and split the document into smaller units), and the `TextConverter` (to convert the txt file into a Document object)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceS7kH6nnNZl"
      },
      "outputs": [],
      "source": [
        "# Initialize the DocumentStore\n",
        "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\", similarity=\"dot_product\", embedding_dim=384)\n",
        "\n",
        "# Initialize the PreProcessor\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=False,\n",
        "    split_by=\"word\",\n",
        "    split_length=100,\n",
        "    split_respect_sentence_boundary=True,\n",
        ")\n",
        "\n",
        "# Initialize the TextConverter\n",
        "text_converter = TextConverter()\n",
        "\n",
        "# Run the TextIndexingPipeline\n",
        "pipeline = TextIndexingPipeline(document_store, text_converter, preprocessor)\n",
        "result = pipeline.run(file_path=\"argilla_cloud.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LstBMR8SnNZm"
      },
      "source": [
        "After having our file prepared, we initialize the retriever and prompt node. In our case, we will use a dense retriever, `EmbeddingRetriever`, which computes the embeddings of our document and those of the query. The most common architecture and the one that we will use in this blog post is `sentence-transformers`, you can find more information about it [here](https://www.davidsbatista.net/blog/2023/10/22/SentenceTransformers/).\n",
        "\n",
        "Regarding the [PromptNode](https://docs.haystack.deepset.ai/docs/prompt_node), we will use the [flan-t5-large](https://huggingface.co/google/flan-t5-large) model, an open-source LLM of Google available in HuggingFaces, although [more](https://docs.haystack.deepset.ai/docs/prompt_node#models) can be used. And it will use the custom [prompt template](https://docs.haystack.deepset.ai/docs/prompt_node#prompttemplates) `rag_prompt` that we have defined.\n",
        "\n",
        "\n",
        ">```python\n",
        "># Use this code if you prefer to use the OpenAI API for the prompt node\n",
        "># Remember to add your OpenAI API key for generation\n",
        ">prompt_node = PromptNode(\n",
        ">    model_name_or_path=\"text-davinci-003\", api_key='api_key', default_prompt_template=rag_prompt\n",
        ">)\n",
        ">```\n",
        "\n",
        "For more information, check the [documentation](https://docs.haystack.deepset.ai/docs/pipelines#querying-pipelines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLBT1DgPnNZm"
      },
      "outputs": [],
      "source": [
        "# Initialize the EmbeddingRetriever\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store, embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        ")\n",
        "document_store.update_embeddings(retriever)\n",
        "\n",
        "# Write the prompt for RAG\n",
        "rag_prompt = PromptTemplate(\n",
        "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
        "            Provide a clear and concise response.\n",
        "            Your answer should be in your own words and be no longer than 50 words.\n",
        "            \\n\\n Related text: {join(documents, delimiter=new_line, pattern=new_line+'Document[$idx]: $content', str_replace={new_line: ' ', '[': '(', ']': ')'})} \\n Question: {query}; Answer: \"\"\",\n",
        "            output_parser=AnswerParser(reference_pattern=r\"Document\\[(\\d+)\\]\"),\n",
        ")\n",
        "\n",
        "# Initialize PromptNode\n",
        "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=rag_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocBIdSmCnNZm"
      },
      "source": [
        "Now, we can run our QA pipeline with the retriever and the prompt node we have initialized using `pipe.run`. To do so, we will create a loop to iterate over the questions and get a list with the answers and the different contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8m_ffy3nNZm"
      },
      "outputs": [],
      "source": [
        "# Create the QA pipeline\n",
        "pipe = Pipeline()\n",
        "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
        "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])\n",
        "\n",
        "# Run the pipeline\n",
        "questions = dataset[\"train\"][\"question\"]\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in tqdm(questions):\n",
        "\n",
        "    # Get the response and save it\n",
        "    response = pipe.run(query=question)\n",
        "    answers.append(response[\"answers\"][0].answer)\n",
        "\n",
        "    # Get the document contexts and save them\n",
        "    prompt = response[\"answers\"][0].meta['prompt']\n",
        "    segments = re.split(r'Document\\[\\d+\\]:', prompt)\n",
        "    document_segments = [segment.strip() for segment in segments[1:]]\n",
        "    contexts.append(document_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XbS-EsunNZm",
        "outputId": "28a7cab4-8d73-4526-f7f7-e42ab7a3fbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the ticketing system used by Argilla for customer support?\n",
            "Answer: volume of records processed.\n",
            "Contexts: [\"In summary, Argilla Cloud's support services are designed to provide timely and efficient assistance for issues of varying severity, ensuring a smooth and reliable user experience. All plans include Monday to Friday during office hours (8am to 17pm CEST) with additional support upon request. The Support Channels and features of each tier are shown below: Starter: Slack Community. Severity 1 - Response time < 4 hours. Severity 2 - Response time < 8 hours. Severity 3 - Response time < 48 hours. Severity 4 not specified. Base: Ticketing System, Severity 1 - Response time < 4 hours.\", \"They have the option to configure settings as per their team's requirements, including assigning datasets to specific workspaces and managing access permissions. Step 5: Training and Support Argilla provides open resources and support to aid in the onboarding process. This includes user manuals, tutorials, and access to our support team for any queries or issues that may arise during the setup and onboarding process. By following these steps, new users can be quickly onboarded and begin using the Argilla Cloud service with minimal downtime.\", \"This process ensures the client administrator has full control over their team's access and can manage their workspace efficiently. Plans The plans for the Argilla Cloud service depend on the volume of records processed, with several tiers available to suit varying needs. Each tier has a corresponding monthly and annual price, with a 10% discount applied to the annual pricing option. The tier selection and associated price will be determined by the client'\"]\n"
          ]
        }
      ],
      "source": [
        "# Question: the query asked the model\n",
        "print(f\"Question: {questions[0]}\")\n",
        "\n",
        "# Answer: the answer generated by the model\n",
        "print(f\"Answer: {answers[0]}\")\n",
        "\n",
        "# Contexts: a list of contexts retrieved by the model\n",
        "print(f\"Contexts: {contexts[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3xTL_cqnNZn"
      },
      "source": [
        "### Create the Argilla Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH1g10QHnNZn"
      },
      "source": [
        "Finally, we got our raw data, so we can create the Argilla datasets using the [FeedbackDatasetTemplates](/practical_guides/create_update_dataset/create_dataset.html#task-templates). In this case, we will use the `FeedbackDataset.for_retrieval_augmented_generation` template for the retrieval storing the query and the contexts and the `FeedbackDataset.for_sentence_similarity` template for the reranking storing of the contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EimmuiX4nNZo"
      },
      "source": [
        "#### For RAG\n",
        "\n",
        "We initialize the template with the default arguments up to the `number_of_retrievals` which will be set to 3 (as we have three contexts) and we add a new metadata property (the source of the contexts, in case we later wanted to compare them with those obtained with another model). Then, we add the records with the fields iterating over the lists we created and push them to Argilla using the `push_to_argilla` method. This will allow us to visualize the dataset in the Argilla UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11_wRyHpnNZo",
        "outputId": "a86354e2-c64c-4633-bfec-1b0561230af7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FeedbackDataset(\n",
              "   fields=[TextField(name='query', title='Query', required=True, type='text', use_markdown=False), TextField(name='retrieved_document_1', title='Retrieved Document 1', required=True, type='text', use_markdown=False), TextField(name='retrieved_document_2', title='Retrieved Document 2', required=False, type='text', use_markdown=False), TextField(name='retrieved_document_3', title='Retrieved Document 3', required=False, type='text', use_markdown=False)]\n",
              "   questions=[RatingQuestion(name='rating_retrieved_document_1', title='Rate the relevance of the Retrieved Document 1 for the query', description='Rate the relevance of the retrieved document.', required=True, type='rating', values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), RatingQuestion(name='rating_retrieved_document_2', title='Rate the relevance of the Retrieved Document 2 for the query', description='Rate the relevance of the retrieved document.', required=False, type='rating', values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), RatingQuestion(name='rating_retrieved_document_3', title='Rate the relevance of the Retrieved Document 3 for the query', description='Rate the relevance of the retrieved document.', required=False, type='rating', values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), TextQuestion(name='response', title='Write a helpful, harmless, accurate response to the query.', description='Write the response to the query.', required=False, type='text', use_markdown=False)]\n",
              "   guidelines=This is a retrieval augmented generation dataset that contains queries and retrieved documents. Please rate the relevancy of retrieved document and write the response to the query in the response field.)\n",
              "   metadata_properties=[])\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the FeedbackDatasetTemplate\n",
        "dataset_rag = rg.FeedbackDataset.for_retrieval_augmented_generation(\n",
        "    number_of_retrievals=3,\n",
        "    rating_scale=10,\n",
        "    use_markdown=False,\n",
        "    guidelines=None,\n",
        "    metadata_properties=None,\n",
        "    vectors_settings=None,\n",
        ")\n",
        "dataset_rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxFvNAhdnNZo",
        "outputId": "104572d7-dab1-484f-dca8-36e656dbf20a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TermsMetadataProperty(name='source', title='Source model', visible_for_annotators=True, type='terms', values=None)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Add the new metadata property\n",
        "metadata = rg.TermsMetadataProperty(name=\"source\", title=\"Source model\")\n",
        "\n",
        "dataset_rag.add_metadata_property(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7DZ2As4nNZo"
      },
      "outputs": [],
      "source": [
        "# Create the proper records\n",
        "records = [\n",
        "    rg.FeedbackRecord(\n",
        "        fields={\"query\": question, \"retrieved_document_1\": context[0], \"retrieved_document_2\": context[1], \"retrieved_document_3\": context[2]},\n",
        "        metadata={\"source\": \"flan-t5-large\"}\n",
        "    )\n",
        "    for question, context in tqdm(zip(questions, contexts))\n",
        "]\n",
        "\n",
        "# Add records to the dataset\n",
        "dataset_rag.add_records(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXj_mGFWnNZo"
      },
      "outputs": [],
      "source": [
        "# Publish the dataset in the Argilla UI\n",
        "dataset_rag = dataset_rag.push_to_argilla(name=\"my_rag_dataset\", workspace=\"admin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IimjrqQNnNZp"
      },
      "source": [
        "![rag_dataset.PNG](attachment:rag_dataset.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWvap3R9nNZp"
      },
      "source": [
        "#### For Sentence Similarity\n",
        "\n",
        "The flow is similar to the previous one. We initialize the template, add the metadata property (in this case, the source would be useful to know the origin of the contexts we're comparing) and, as we want to compare the three sentences, but the cross-encoder only supports a comparison of two sentences, we iterate over the three contexts to match them as 1-2,¬†1-3¬†and¬†2-3. Then, we add the records, and push it to Argilla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TnHbqfMnNZp",
        "outputId": "53a1e4a8-ceb3-4637-bd90-8607ec566164"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FeedbackDataset(\n",
              "   fields=[TextField(name='sentence-1', title='Sentence-1', required=True, type='text', use_markdown=True), TextField(name='sentence-2', title='Sentence-2', required=True, type='text', use_markdown=True)]\n",
              "   questions=[RatingQuestion(name='similarity', title='Similarity', description='Rate the similarity between the two sentences.', required=True, type='rating', values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])]\n",
              "   guidelines=This is a sentence similarity dataset that contains two sentences. Please rate the similarity between the two sentences.)\n",
              "   metadata_properties=[])\n",
              ")"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize the FeedbackDatasetTemplate\n",
        "dataset_ssim = rg.FeedbackDataset.for_sentence_similarity(\n",
        "    rating_scale=10,\n",
        "    use_markdown=True,\n",
        "    guidelines=None,\n",
        "    metadata_properties=None,\n",
        "    vectors_settings=None,\n",
        ")\n",
        "dataset_ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keoT_eyvnNZp",
        "outputId": "065df299-6aca-4c35-9ba7-d3a09a317288"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TermsMetadataProperty(name='sources', title='Model sources', visible_for_annotators=True, type='terms', values=['flan-t5-large/flan-t5-large'])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Add the new metadata property\n",
        "metadata = rg.TermsMetadataProperty(name=\"sources\", title=\"Model sources\", values=[\"flan-t5-large/flan-t5-large\"])\n",
        "\n",
        "dataset_ssim.add_metadata_property(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSkUL_UInNZq"
      },
      "outputs": [],
      "source": [
        "# Create the proper records\n",
        "records = []\n",
        "for context in tqdm(contexts):\n",
        "    for i in range(len(context)):\n",
        "        for j in range(i + 1, len(context)):\n",
        "            record = rg.FeedbackRecord(\n",
        "                fields={\"sentence1\": context[i], \"sentence2\": context[j]},\n",
        "                metadata={\"sources\": \"flan-t5-large/flan-t5-large\"}\n",
        "            )\n",
        "            records.append(record)\n",
        "\n",
        "# Add records to the dataset\n",
        "dataset_ssim.add_records(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svn4KqPDnNZq"
      },
      "outputs": [],
      "source": [
        "# Publish the dataset in the Argilla UI\n",
        "dataset_ssim = dataset_ssim.push_to_argilla(name=\"my_ssim_dataset\", workspace=\"admin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktf_8RPBnNZq"
      },
      "source": [
        "![ssim_dataset.PNG](attachment:ssim_dataset.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxpP-7nbnNZq"
      },
      "source": [
        "## Fine-tuning the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWLTjiaYnNZq"
      },
      "source": [
        "To improve the retrieval model, we need to [fine-tune a sentence similarity model](../../practical_guides/fine_tune.html#sentence-similarity), which compares the query embeddings and the context embeddings of the documents retrieved in the case of the bi-encoder and rank the contexts in the case of the cross-encoder. In this tutorial, we will fine-tune both models using the [ArgillaTrainer](../../practical_guides/fine_tune.html#argillatrainer) with the datasets we have created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1HNDs-4nNZr"
      },
      "source": [
        "### Bi-Encoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C_KohA6nNZr"
      },
      "source": [
        "First, we prepare the data for fine-tuning the bi-encoder. Instead of using the default `TrainingTask.for_sentence_similarity`, we use the `formatting_func` so that we compare two sentences (the query and each of the contexts) and we add the rating value indicated by the annotators using the Argilla UI to be more precise.\n",
        "\n",
        "Using the `ArgillaTrainer` is really easy to fine-tune a sentence similarity model. We just need to initialize the trainer and call `train`. To set if using the `bi-encoder` or the `cross-encoder`, we just need to set `framework_kwargs={\"cross_encoder\": False}` or `framework_kwargs={\"cross_encoder\": True}`. Check the [documentation](/practical_guides/fine_tune.html#training-configs) for further customization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qpKeYMknNZr"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from Argilla\n",
        "dataset_rag = rg.FeedbackDataset.from_argilla(\"my_rag_dataset\", workspace=\"admin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKsHlAzQnNZr"
      },
      "outputs": [],
      "source": [
        "# Define the training task using the formatting function\n",
        "def formatting_func(sample):\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for i in range(1, 4):\n",
        "        record = {\"sentence-1\": sample[\"query\"], \"sentence-2\": sample[f\"retrieved_document_{i}\"]}\n",
        "        values = [resp[\"value\"] for resp in sample[f\"rating_retrieved_document_{i}\"]]\n",
        "        label = int(values[0])\n",
        "        record[\"label\"] = label\n",
        "        records.append(record)\n",
        "\n",
        "    return records\n",
        "\n",
        "task = TrainingTask.for_sentence_similarity(formatting_func=formatting_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wO-tdoInNZr"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the bi-encoder\n",
        "trainer_bi = ArgillaTrainer(\n",
        "    dataset=dataset_rag,\n",
        "    task=task,\n",
        "    framework=\"sentence-transformers\",\n",
        "    framework_kwargs={\"cross_encoder\": False}\n",
        ")\n",
        "trainer_bi.train(output_dir=\"my_bi_sentence_transformer_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS2l9iyZnNZr"
      },
      "source": [
        "### Cross-Encoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DknMvnqnNZs"
      },
      "source": [
        "Our initial task is to prepare the data for fine-tuning the cross-encoder. Within the `TrainingTask.for_sentence_similarity function`, we explicitly focus on comparing only two contexts that the model will subsequently rank and add the rating annotation. This approach ensures that when the retrieval model retrieves documents, the cross-encoder can effectively rank them and return the most similar one.\n",
        "\n",
        "Then, we initialize and run the trainer as we did with the bi-encoder. The only difference is that we set ``framework_kwargs={\"cross_encoder\": True}``. Check the [documentation](/practical_guides/fine_tune.html#training-configs) for further customization.\n",
        "\n",
        "> üí≠ Remember that the ``cross-encoder`` can not be trained with triplets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7CTGWRinNZs"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from Argilla\n",
        "dataset_ssim = rg.FeedbackDataset.from_argilla(\"my_ssim_dataset\", workspace=\"admin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seNXfuFWnNZs"
      },
      "outputs": [],
      "source": [
        "# Define the training task\n",
        "task = TrainingTask.for_sentence_similarity(\n",
        "    texts=[dataset_ssim.field_by_name(\"sentence-1\"), dataset_ssim.field_by_name(\"sentence-2\")],\n",
        "    label=dataset_ssim.question_by_name(\"similarity\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcLh1DL5nNZs"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the cross-encoder\n",
        "trainer_cross = ArgillaTrainer(\n",
        "    dataset=dataset_ssim,\n",
        "    task=task,\n",
        "    framework=\"sentence-transformers\",\n",
        "    framework_kwargs={\"cross_encoder\": True}\n",
        ")\n",
        "trainer_cross.train(output_dir=\"my_cross_sentence_transformer_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udqObq6gnNZs"
      },
      "source": [
        "# Using our Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYe4IEVMnNZt"
      },
      "source": [
        "We are now in the final step, so we are going to use our fine-tuned models again with Haystack so that we can get new and better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nCyX7_fnNZt"
      },
      "source": [
        "To do so, we will use the same questions and documents for RAG as previously. If you have completed all the preceding steps, then you can continue.\n",
        "\n",
        "> Remember, it's unnecessary to reinitialize the document_store if the same documents are being used. However, ensure that the embeddings_dim align with those from our fine-tuned models. If there's a mismatch, reinitialize the document_store  with the correct values and run the TextIndexingPipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBExlkyTnNZt"
      },
      "source": [
        "First, we will initialize the `EmbeddingRetriever` with our fine-tuned bi-encoder model and update the embeddings. Then, we should add the [ranker](https://docs.haystack.deepset.ai/docs/ranker#sentencetransformersranker) by initializing the `SentenceTransformersRanker` with out fine-tuned cross-encoder model that will be used to boost our RAG model following the same idea as the diagram at the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv61rZLSnNZt"
      },
      "outputs": [],
      "source": [
        "# Initialize the EmbeddingRetriever with out model\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store, embedding_model=\"my_bi_sentence_transformer_model\"\n",
        ")\n",
        "document_store.update_embeddings(retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBUr4vCHnNZt"
      },
      "outputs": [],
      "source": [
        "# Initialize the SentenceTransformersRanker with out model\n",
        "ranker = SentenceTransformersRanker(model_name_or_path=\"my_cross_sentence_transformer_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZBSjw0nNZu"
      },
      "source": [
        "As before, we should also initialize the prompt node with the `flan-t5-large` model and the `rag_prompt` template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RT5xC-lnNZu"
      },
      "outputs": [],
      "source": [
        "# Write the prompt for RAG\n",
        "rag_prompt = PromptTemplate(\n",
        "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
        "            Provide a clear and concise response.\n",
        "            Your answer should be in your own words and be no longer than 50 words.\n",
        "            \\n\\n Related text: {join(documents, delimiter=new_line, pattern=new_line+'Document[$idx]: $content', str_replace={new_line: ' ', '[': '(', ']': ')'})} \\n Question: {query}; Answer: \"\"\",\n",
        "            output_parser=AnswerParser(reference_pattern=r\"Document\\[(\\d+)\\]\"),\n",
        ")\n",
        "\n",
        "# Initialize PromptNode\n",
        "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=rag_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLhRGuqEnNZu"
      },
      "source": [
        "Now, let's create the final pipeline concatenating all the components! And run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eer4lYmynNZu"
      },
      "outputs": [],
      "source": [
        "# Create the QA pipeline\n",
        "pipe = Pipeline()\n",
        "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
        "pipe.add_node(component=ranker, name=\"ranker\", inputs=[\"retriever\"])\n",
        "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"ranker\"])\n",
        "\n",
        "# Run the pipeline\n",
        "questions = dataset[\"train\"][\"question\"]\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in tqdm(questions):\n",
        "\n",
        "    # Get the response and save it\n",
        "    response = pipe.run(query=question)\n",
        "    answers.append(response[\"answers\"][0].answer)\n",
        "\n",
        "    # Get the document context and save it\n",
        "    prompt = response[\"answers\"][0].meta['prompt']\n",
        "    segments = re.split(r'Document\\[\\d+\\]:', prompt)\n",
        "    document_segments = [segment.strip() for segment in segments[1:]]\n",
        "    contexts.append(document_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGdEQYbCnNZv",
        "outputId": "4b45ae6a-942a-4ed8-f659-68f36bc7b1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the ticketing system used by Argilla for customer support?\n",
            "Answer: Argilla Cloud's support services are designed to provide timely and efficient assistance for issues of varying severity, ensuring a smooth and reliable user experience. All plans include Monday to Friday during office hours (8am to 17pm CEST) with additional support upon request.\n",
            "Contexts: [\"In summary, Argilla Cloud's support services are designed to provide timely and efficient assistance for issues of varying severity, ensuring a smooth and reliable user experience. All plans include Monday to Friday during office hours (8am to 17pm CEST) with additional support upon request. The Support Channels and features of each tier are shown below: Starter: Slack Community. Severity 1 - Response time < 4 hours. Severity 2 - Response time < 8 hours. Severity 3 - Response time < 48 hours. Severity 4 not specified. Base: Ticketing System, Severity 1 - Response time < 4 hours.\", 'This documents an overview of the Argilla Cloud service - a comprehensive Software as a Service (SaaS) solution for data labeling and curation. The service is specifically designed to meet the needs of businesses seeking a reliable, secure, and user-friendly platform for data management. The key components of our service include advanced security measures, robust data backup and recovery protocols, flexible pricing options, and dedicated customer support. The onboarding process is efficient, enabling clients to start using the service within one business day.', \"They have the option to configure settings as per their team's requirements, including assigning datasets to specific workspaces and managing access permissions. Step 5: Training and Support Argilla provides open resources and support to aid in the onboarding process. This includes user manuals, tutorials, and access to our support team for any queries or issues that may arise during the setup and onboarding process. By following these steps, new users can be quickly on\"]\n"
          ]
        }
      ],
      "source": [
        "# Question: the query asked the model\n",
        "print(f\"Question: {questions[0]}\")\n",
        "\n",
        "# Answer: the answer generated by the model\n",
        "print(f\"Answer: {answers[0]}\")\n",
        "\n",
        "# Contexts: a list of contexts retrieved by the model\n",
        "print(f\"Contexts: {contexts[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3fBgivNnNZv"
      },
      "source": [
        "And as we can see, the results are much better than before and the ranker provided us the best context! üëè"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXX_23EAnNZv"
      },
      "source": [
        "Now, all that's left is for you to experiment with various parameters ‚Äì I'm confident you can enhance these results even further! üí™"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "argilla-docs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}